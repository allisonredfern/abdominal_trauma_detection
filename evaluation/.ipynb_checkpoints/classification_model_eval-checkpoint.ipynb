{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741749f6-4f87-4b0b-b1f7-58c0905be3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.functional import relu\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "import torch\n",
    "from skimage import color\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from PIL import Image, ImageOps\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score, confusion_matrix\n",
    "from torchvision.models.video import r3d_18\n",
    "from torchvision.transforms import RandomHorizontalFlip, RandomVerticalFlip, RandomRotation\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1159ed0-b155-47bb-88d4-1330ea54f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('y')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0b6ff-f546-47bd-adce-c1248f6db485",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13bac31f-dec6-4164-b388-954d06054054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation3D(object):\n",
    "    def __init__(self, degrees):\n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (numpy.ndarray): Input 3D image numpy array.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Randomly rotated image.\n",
    "        \"\"\"\n",
    "        # Randomly select the rotation angle\n",
    "        angle = np.random.uniform(-self.degrees, self.degrees)\n",
    "\n",
    "        # Perform rotation\n",
    "        rotated_img = np.array([np.rot90(slice, k=int(angle / 90)) for slice in img])\n",
    "\n",
    "        return rotated_img\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    RandomRotation3D(degrees=5)\n",
    "])\n",
    "\n",
    "\n",
    "class AbdominalDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label = self.data_frame['target']\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir,\n",
    "                                self.data_frame.iloc[idx, -2])\n",
    "\n",
    "        image = np.load(img_name)\n",
    "        label = self.label[idx]\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        label = label.astype(np.float32)\n",
    "\n",
    "        resized_image = F.interpolate(torch.tensor(image).unsqueeze(0).unsqueeze(0), size=(128, 128, 128), mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "\n",
    "        # Select only the last 64 slices: \n",
    "        resized_image = resized_image[-64:, :, :]\n",
    "\n",
    "        if self.transform:\n",
    "            resized_image = self.transform(resized_image)\n",
    "            \n",
    "        # Normalize the image\n",
    "        min_image = np.min(resized_image)\n",
    "        max_image = np.max(resized_image)\n",
    "        resized_image = (resized_image - min_image) / (max_image - min_image + 1e-4)\n",
    "\n",
    "    \n",
    "        # Convert to tensors\n",
    "        image = torch.tensor(resized_image).float().unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        sample = {'x': image, 'y': label}\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b9013-90ee-4c51-bc7e-e78627b489b7",
   "metadata": {},
   "source": [
    "## ResNet18-2D with LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637846cd-a0c5-43a9-824e-c8758ad915b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLSTM(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super(ResNetLSTM, self).__init__()\n",
    "        # Load pre-trained ResNet18\n",
    "        self.resnet = models.resnet18(weights='DEFAULT')\n",
    "        \n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Change the last fully connected layer\n",
    "        self.resnet.fc = nn.Linear(512, 128)\n",
    "        \n",
    "        # Define LSTM parameters\n",
    "        input_size = 128\n",
    "        hidden_size = 64\n",
    "        num_layers = 1\n",
    "        bidirectional = True\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, 64)  # Additional dense layer\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate*3)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2 if bidirectional else hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, num_slices, h, w = x.size()\n",
    "        \n",
    "        # Pass through ResNet\n",
    "        x = x.view(batch_size * num_slices, 1, h, w)\n",
    "        x = self.resnet(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(batch_size, num_slices, -1)\n",
    "            \n",
    "        # Pass through LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "\n",
    "        # Global average pooling across slices\n",
    "        x, _ = torch.max(x, dim=1)\n",
    "\n",
    "        # Apply Batch Norm\n",
    "        x = self.batch_norm(x)\n",
    "\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Pass through additional dense layer\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Pass through final dense layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x.squeeze()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86591ab2-3062-455c-a66e-3aaf37eebc07",
   "metadata": {},
   "source": [
    "## ResNet18 3D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d324c5-5de8-484d-8e4c-920e13678ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18_3D(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ResNet18_3D, self).__init__()\n",
    "        self.resnet18_3d = r3d_18(weights='DEFAULT')\n",
    "\n",
    "        # Modify stem layer to accept single-channel input\n",
    "        self.resnet18_3d.stem = nn.Conv3d(1, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
    "        \n",
    "        # Change the last fully connected layer\n",
    "        self.resnet18_3d.fc = nn.Linear(self.resnet18_3d.fc.in_features, 1)\n",
    "        \n",
    "        # Add Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Add Batch Normalization\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.BatchNorm3d(512)\n",
    "        ])\n",
    "        \n",
    "        # Adaptive average pooling\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        # Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18_3d.stem(x)\n",
    "        x = self.batch_norms[0](x)\n",
    "        x = self.resnet18_3d.layer1(x)\n",
    "        x = self.batch_norms[1](x)\n",
    "        x = self.resnet18_3d.layer2(x)\n",
    "        x = self.batch_norms[2](x)\n",
    "        x = self.resnet18_3d.layer3(x)\n",
    "        x = self.batch_norms[3](x)\n",
    "        x = self.resnet18_3d.layer4(x)\n",
    "        x = self.batch_norms[4](x)\n",
    "        x = self.adaptive_avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.resnet18_3d.fc(x)\n",
    "        \n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3d958-27c1-4024-b134-93c52abdbf3f",
   "metadata": {},
   "source": [
    "## Load Model Weights & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff6b7662-e7be-4336-aa5c-9c1b09ccb061",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet2d_LSTM = ResNetLSTM().to(device)\n",
    "resnet2d_LSTM_cropped = ResNetLSTM().to(device)\n",
    "resnet3d = ResNet18_3D().to(device)\n",
    "resnet3d_cropped = ResNet18_3D().to(device)\n",
    "resnet3d2 = ResNet18_3D().to(device)\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "model_directory = os.path.join(current_directory, '..', 'models')\n",
    "model_archive_directory = os.path.join(model_directory, 'MIN_LOSS_FINAL_NO_OUTPUTS')\n",
    "\n",
    "# Load the weights for resnet2d_LSTM\n",
    "resnet_2d_LSTM_weights = torch.load(os.path.join(model_directory, 'classification_model_resnet18_LSTM_binary_0_9_final.pth'))\n",
    "resnet2d_LSTM.load_state_dict(resnet_2d_LSTM_weights)\n",
    "\n",
    "# Load the weights for resnet2d_LSTM (cropped version)\n",
    "resnet_2d_LSTM_cropped_weights = torch.load(os.path.join(model_directory, 'classification_model_resnet18_LSTM_cropped_binary_0_9_final.pth'))\n",
    "resnet2d_LSTM_cropped.load_state_dict(resnet_2d_LSTM_cropped_weights)\n",
    "\n",
    "# Load the weights for resnet3d\n",
    "resnet_3d_weights = torch.load(os.path.join(model_directory, 'classification_model_resnet18_binary_0_9_final.pth'))\n",
    "resnet3d.load_state_dict(resnet_3d_weights)\n",
    "\n",
    "# Load the weights for resnet3d (cropped version)\n",
    "resnet_3d_cropped_weights = torch.load(os.path.join(model_directory, 'classification_model_resnet18_cropped_binary_0_9_final.pth'))\n",
    "resnet3d_cropped.load_state_dict(resnet_3d_cropped_weights)\n",
    "\n",
    "# resnet_3d_weights_option2 = torch.load(os.path.join(model_archive_directory, 'classification_model_resnet18_binary_0_9_3.pth'))\n",
    "# resnet3d2.load_state_dict(resnet_3d_weights_option2)\n",
    "\n",
    "classification_models = [resnet2d_LSTM_cropped, resnet2d_LSTM, resnet3d_cropped, resnet3d]# , resnet3d2]\n",
    "model_names = ['2D ResNet18 + LSTM - Cropped', '2D ResNet18 + LSTM - Uncropped', '3D ResNet18 - Cropped', '3D ResNet18 - Uncropped'] #, '3D ResNet18 - Uncropped2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1455550-9b46-49a0-9f87-d00e20b9648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "data_directory = os.path.join(current_directory, '..', 'data')\n",
    "\n",
    "bs = 8\n",
    "\n",
    "uncropped_dataset = AbdominalDataset(os.path.join(data_directory, 'test_binary.csv'),'/vast/amr10211/deep_learning_final_project/images_preprocessed/')\n",
    "cropped_dataset = AbdominalDataset(os.path.join(data_directory, 'test_binary.csv'),'/vast/amr10211/deep_learning_final_project/masked_images/')\n",
    "\n",
    "uncropped_dataloader = DataLoader(uncropped_dataset, batch_size=bs, shuffle=False)\n",
    "cropped_dataloader = DataLoader(cropped_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "dataloaders = [cropped_dataloader, uncropped_dataloader, cropped_dataloader, uncropped_dataloader] #,uncropped_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ea3a9-851b-4a0a-ab10-29f293edadba",
   "metadata": {},
   "source": [
    "## Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d02b5d-20d9-4520-9a81-cea8e2e8231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Results:\n",
    "\n",
    "# Initialize dictionaries to store dataframes for each model\n",
    "label_dfs = {}\n",
    "prob_dfs = {}\n",
    "pred_dfs = {}\n",
    "\n",
    "# Iterate over models\n",
    "for i, model in enumerate(classification_models):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_labels = np.array([])\n",
    "    all_probs = np.array([])  # To store predicted probabilities\n",
    "    all_preds = np.array([])  # To store predicted class labels\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dataloader = dataloaders[i]\n",
    "        for data in dataloader:\n",
    "            image = data['x'].to(device)\n",
    "            labels = data['y'].to(device)\n",
    "            outputs = model(image)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.round(probs) \n",
    "            all_labels = np.concatenate((all_labels, labels.cpu().numpy()), axis=0)\n",
    "            all_probs = np.concatenate((all_probs, probs.cpu().numpy()), axis=0)\n",
    "            all_preds = np.concatenate((all_preds, preds.cpu().numpy()), axis=0)\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    # Create dataframes\n",
    "    label_df = pd.DataFrame(all_labels, columns=['Labels'])\n",
    "    prob_df = pd.DataFrame(all_probs, columns=['Probabilities'])\n",
    "    pred_df = pd.DataFrame(all_preds, columns=['Predictions'])\n",
    "    \n",
    "    # Save dataframes in dictionaries\n",
    "    label_dfs[model_names[i]] = label_df\n",
    "    prob_dfs[model_names[i]] = prob_df\n",
    "    pred_dfs[model_names[i]] = pred_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0799e4-95cd-4b4e-8c32-7be9dfabb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(label_dfs, f)\n",
    "\n",
    "with open('prob_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(prob_dfs, f)\n",
    "\n",
    "with open('pred_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(pred_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daad9cd-aa4a-46bd-b149-ba14156e2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_dfs.pkl', 'rb') as f:\n",
    "    label_dfs = pickle.load(f)\n",
    "\n",
    "with open('prob_dfs.pkl', 'rb') as f:\n",
    "    prob_dfs = pickle.load(f)\n",
    "\n",
    "with open('pred_dfs.pkl', 'rb') as f:\n",
    "    pred_dfs = pickle.load(f)\n",
    "\n",
    "colors = ['indianred', 'sandybrown', 'cornflowerblue', 'mediumpurple'] #, 'green']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36603e-bbcf-4b64-881a-358f0bdd1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, (model_name, label_df) in enumerate(label_dfs.items()):\n",
    "    total_samples = len(label_df)\n",
    "    \n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    prob_df = prob_dfs[model_name]\n",
    "    pred_df = pred_dfs[model_name]\n",
    "\n",
    "    # Calculate AUC score\n",
    "    fprs, tprs, _ = roc_curve(label_df.values, prob_df.values)\n",
    "    auc_score = auc(fprs, tprs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fprs, tprs, color=colors[i], lw=2, label='{} (AUC = {:.2f})'.format(model_name, auc_score))\n",
    "\n",
    "# Add labels and titles for ROC curve\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC Curve', fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487e178-6971-4a18-a9a2-3549fe6e292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUPRC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, (model_name, label_df) in enumerate(label_dfs.items()):\n",
    "    total_samples = len(label_df)\n",
    "    \n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    prob_df = prob_dfs[model_name]\n",
    "    pred_df = pred_dfs[model_name]\n",
    "\n",
    "    # Calculate Precision and Recall\n",
    "    precision, recall, _ = precision_recall_curve(label_df.values, prob_df.values)\n",
    "    auprc_score = average_precision_score(label_df.values, prob_df.values)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.step(recall, precision, color=colors[i], where='post', label='{} (AUPRC = {:.2f})'.format(model_name, auprc_score))\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.1, color=colors[i])\n",
    "\n",
    "# Add labels and titles for Precision-Recall curve\n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('Precision-Recall curve', fontsize=14)\n",
    "plt.legend(loc='upper right', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f2946-a0b9-40e3-a395-eb68b86f9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ad380-43ad-40f1-bcf1-d597dca24104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f1_scores = []\n",
    "model_names = []\n",
    "new_model_names = ['2D ResNet18 + LSTM\\nCropped', '2D ResNet18 + LSTM\\nUncropped', '3D ResNet18\\nCropped', '3D ResNet18\\nUncropped']\n",
    "\n",
    "# Iterate over model names and label dataframes\n",
    "for model_name, label_df in label_dfs.items():\n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    pred_df = pred_dfs[model_name]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(label_df.values, pred_df.values, average='binary')\n",
    "    \n",
    "    # Append F1 score and model name to lists\n",
    "    f1_scores.append(f1)\n",
    "    model_names.append(model_name)\n",
    "\n",
    "# Plot bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(new_model_names, f1_scores, color=colors)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('F1 Score', fontsize=14)\n",
    "plt.title('F1 Score of Each Model', fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=9)\n",
    "plt.ylim(0, 0.4)\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a06af-5f17-4365-b958-b13746a7be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPR\n",
    "fpr_scores = []\n",
    "fnr_scores = []\n",
    "\n",
    "\n",
    "# Calculate FPR for each model\n",
    "for model_name, label_df in label_dfs.items():\n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    pred_df = pred_dfs[model_name]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(label_df.values, pred_df.values).ravel()\n",
    "    \n",
    "    # Calculate FPR\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "    \n",
    "    # Append FPR to the list\n",
    "    fpr_scores.append(fpr)\n",
    "    fnr_scores.append(fnr)\n",
    "\n",
    "# Set figure size and width of bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define bar width\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set up x values for the bars\n",
    "x = np.arange(len(model_names))\n",
    "\n",
    "# Plot FNR bars\n",
    "plt.bar(x - bar_width/2, fnr_scores, width=bar_width, color='lightsteelblue', label='False Negative Rate')\n",
    "\n",
    "# Plot FPR bars\n",
    "plt.bar(x + bar_width/2, fpr_scores, width=bar_width, color='thistle', label='False Positive Rate')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Rate', fontsize=14)\n",
    "plt.title('False Negative Rate and False Positive Rate of Each Model \\nAll Data', fontsize=14)\n",
    "plt.ylim(0,1)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(x, model_names, rotation=45, fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels to each bar\n",
    "for i, score in enumerate(fnr_scores):\n",
    "    plt.text(i - bar_width/2, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=14)\n",
    "\n",
    "for i, score in enumerate(fpr_scores):\n",
    "    plt.text(i + bar_width/2, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa0568-bfaa-4466-b659-bc7d16c44048",
   "metadata": {},
   "source": [
    "## Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ecf09f-dd98-478d-9eec-94dba181a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbdominalDatasetInterpretability(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label = self.data_frame['target']\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir,\n",
    "                                self.data_frame.iloc[idx, -2])\n",
    "\n",
    "        image = np.load(img_name)\n",
    "        label = self.label[idx]\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        label = label.astype(np.float32)\n",
    "\n",
    "        resized_image = F.interpolate(torch.tensor(image).unsqueeze(0).unsqueeze(0), size=(128, 128, 128), mode='trilinear', align_corners=False).squeeze().numpy()\n",
    "\n",
    "        # Select only the last 64 slices: \n",
    "        resized_image = resized_image[-64:, :, :]\n",
    "            \n",
    "        # Normalize the image\n",
    "        min_image = np.min(resized_image)\n",
    "        max_image = np.max(resized_image)\n",
    "        resized_image = (resized_image - min_image) / (max_image - min_image + 1e-4)\n",
    "\n",
    "    \n",
    "        # Convert to tensors\n",
    "        image = torch.tensor(resized_image).float().unsqueeze(0)\n",
    "        label = torch.tensor(label).long()\n",
    "        series_id = torch.tensor(self.data_frame.loc[idx, 'series_id'])\n",
    "        injury_situation = torch.tensor(self.data_frame.loc[idx, 'injury_situation'])\n",
    "\n",
    "        sample = {'x': image, 'y': label, 'series_id': series_id, 'injury_situation': injury_situation}\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404977e-96ce-4429-aa5f-0d80b379dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "data_directory = os.path.join(current_directory, '..', 'data')\n",
    "\n",
    "bs = 8\n",
    "\n",
    "uncropped_interpretability_dataset = AbdominalDatasetInterpretability(os.path.join(data_directory, 'test_interpretability.csv'),'/vast/amr10211/deep_learning_final_project/images_preprocessed/')\n",
    "cropped_interpretability_dataset = AbdominalDatasetInterpretability(os.path.join(data_directory, 'test_interpretability.csv'),'/vast/amr10211/deep_learning_final_project/masked_images/')\n",
    "\n",
    "uncropped_interpretability_dataloader = DataLoader(uncropped_interpretability_dataset, batch_size=bs, shuffle=False)\n",
    "cropped_interpretability_dataloader = DataLoader(cropped_interpretability_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "interpretability_dataloaders = [cropped_interpretability_dataloader, uncropped_interpretability_dataloader, cropped_interpretability_dataloader, uncropped_interpretability_dataloader, uncropped_interpretability_dataloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb615458-4f40-4c04-a00f-7e7900dc8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Results:\n",
    "\n",
    "# Initialize dictionaries to store dataframes for each model\n",
    "interpretability_label_dfs = {}\n",
    "interpretability_prob_dfs = {}\n",
    "interpretability_pred_dfs = {}\n",
    "\n",
    "# Iterate over models\n",
    "for i, model in enumerate(classification_models):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    \n",
    "    all_labels = np.array([])\n",
    "    all_probs = np.array([]) \n",
    "    all_preds = np.array([])\n",
    "    all_series_ids = np.array([])\n",
    "    all_injury_situations = np.array([])\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        dataloader = interpretability_dataloaders[i]\n",
    "        print(i)\n",
    "        for data in dataloader:\n",
    "            image = data['x'].to(device)\n",
    "            labels = data['y'].to(device)\n",
    "            series_id = data['series_id'].to(device)\n",
    "            injury_situation = data['injury_situation'].to(device)\n",
    "            outputs = model(image)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.round(probs) \n",
    "            all_labels = np.concatenate((all_labels, labels.cpu().numpy()), axis=0)\n",
    "            all_probs = np.concatenate((all_probs, probs.cpu().numpy()), axis=0)\n",
    "            all_preds = np.concatenate((all_preds, preds.cpu().numpy()), axis=0)\n",
    "            all_series_ids = np.concatenate((all_series_ids, series_id.cpu().numpy()), axis=0)\n",
    "            all_injury_situations = np.concatenate((all_injury_situations, injury_situation.cpu().numpy()), axis=0)\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    # Create dataframes\n",
    "    interpretability_label_df = pd.DataFrame({'Labels': all_labels, 'series_id': all_series_ids, 'injury_situation': all_injury_situations})\n",
    "    interpretability_prob_df = pd.DataFrame({'Probabilities': all_probs, 'series_id': all_series_ids, 'injury_situation': all_injury_situations})\n",
    "    interpretability_pred_df = pd.DataFrame({'Predictions': all_preds, 'series_id': all_series_ids, 'injury_situation': all_injury_situations})\n",
    "    \n",
    "    # Save dataframes in dictionaries\n",
    "    interpretability_label_dfs[model_names[i]] = interpretability_label_df\n",
    "    interpretability_prob_dfs[model_names[i]] = interpretability_prob_df\n",
    "    interpretability_pred_dfs[model_names[i]] = interpretability_pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bb91c-2336-4aa6-8e27-acba5fca13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('interpretability_label_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(interpretability_label_dfs, f)\n",
    "\n",
    "with open('interpretability_prob_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(interpretability_prob_dfs, f)\n",
    "\n",
    "with open('interpretability_pred_dfs.pkl', 'wb') as f:\n",
    "    pickle.dump(interpretability_pred_dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610a387-e054-4eb4-a8ce-36e1a0818e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('interpretability_label_dfs.pkl', 'rb') as f:\n",
    "    interpretability_label_dfs = pickle.load(f)\n",
    "\n",
    "with open('interpretability_prob_dfs.pkl', 'rb') as f:\n",
    "    interpretability_prob_dfs = pickle.load(f)\n",
    "\n",
    "with open('interpretability_pred_dfs.pkl', 'rb') as f:\n",
    "    interpretability_pred_dfs = pickle.load(f)\n",
    "\n",
    "colors = ['indianred', 'sandybrown', 'cornflowerblue', 'mediumpurple', 'red']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65fe21-6ec1-4a92-a1ff-b2f9df033edc",
   "metadata": {},
   "source": [
    "### At least one other injury:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f49a3-36b3-49be-99d0-f66990af5c02",
   "metadata": {},
   "source": [
    "0: Everything healthy\n",
    "1: Liver healthy, but at least one other not\n",
    "2: Liver injured, and all others healthy\n",
    "3: Liver injured, and at least one other injured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d36bc-59fd-4ce3-9a1a-36a0c4785f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_situation_values = [1, 3]\n",
    "\n",
    "organs_injured_label_dfs = {}\n",
    "organs_injured_prob_dfs = {}\n",
    "organs_injured_pred_dfs = {}\n",
    "\n",
    "for model_name, label_df in interpretability_label_dfs.items():\n",
    "    organs_injured_label_dfs[model_name] = label_df[label_df['injury_situation'].isin(injury_situation_values)]\n",
    "\n",
    "for model_name, prob_df in interpretability_prob_dfs.items():\n",
    "    organs_injured_prob_dfs[model_name] = prob_df[prob_df['injury_situation'].isin(injury_situation_values)]\n",
    "\n",
    "for model_name, pred_df in interpretability_pred_dfs.items():\n",
    "    organs_injured_pred_dfs[model_name] = pred_df[pred_df['injury_situation'].isin(injury_situation_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2d5dd-7fbe-491c-ae93-a44a51d48a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC Curve\n",
    "for i, (model_name, label_df) in enumerate(organs_injured_label_dfs.items()):\n",
    "    total_samples = len(label_df)\n",
    "    \n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    prob_df = organs_injured_prob_dfs[model_name]\n",
    "    pred_df = organs_injured_pred_dfs[model_name]\n",
    "\n",
    "    # Calculate AUC score\n",
    "    fprs, tprs, _ = roc_curve(label_df['Labels'], prob_df['Probabilities'])\n",
    "    auc_score = auc(fprs, tprs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fprs, tprs, color=colors[i], lw=2, label='{} (AUC = {:.2f})'.format(model_name, auc_score))\n",
    "\n",
    "# Add labels and titles for ROC curve\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ed804-7156-4826-b92f-571f46ec8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUPRC Curve\n",
    "for i, (model_name, label_df) in enumerate(organs_injured_label_dfs.items()):\n",
    "    total_samples = len(label_df)\n",
    "    \n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    prob_df = organs_injured_prob_dfs[model_name]\n",
    "    pred_df = organs_injured_pred_dfs[model_name]\n",
    "\n",
    "    # Calculate Precision and Recall\n",
    "    precision, recall, _ = precision_recall_curve(label_df['Labels'], prob_df['Probabilities'])\n",
    "    auprc_score = average_precision_score(label_df['Labels'], prob_df['Probabilities'])\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.step(recall, precision, color=colors[i], where='post', label='{} (AUPRC = {:.2f})'.format(model_name, auprc_score))\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.1, color=colors[i])\n",
    "\n",
    "# Add labels and titles for Precision-Recall curve\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ff4423-956c-410f-8949-83e7cb9fca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f1_scores = []\n",
    "model_names = []\n",
    "\n",
    "# Iterate over model names and label dataframes\n",
    "for model_name, label_df in organs_injured_label_dfs.items():\n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    pred_df = organs_injured_pred_dfs[model_name]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(label_df['Labels'], pred_df['Predictions'], average='binary')\n",
    "    \n",
    "    # Append F1 score and model name to lists\n",
    "    f1_scores.append(f1)\n",
    "    model_names.append(model_name)\n",
    "\n",
    "# Plot bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "bars = plt.bar(model_names, f1_scores, color=colors)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score of Each Model')\n",
    "plt.ylim(0, 0.5)\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01, f'{score:.2f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425038f3-77a3-451e-bf1b-806c5d9e7545",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_scores = []\n",
    "fnr_scores = []\n",
    "\n",
    "\n",
    "# Calculate FPR for each model\n",
    "for model_name, label_df in organs_injured_label_dfs.items():\n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    pred_df = organs_injured_pred_dfs[model_name]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(label_df['Labels'], pred_df['Predictions']).ravel()\n",
    "    \n",
    "    # Calculate FPR\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "    \n",
    "    # Append FPR to the list\n",
    "    fpr_scores.append(fpr)\n",
    "    fnr_scores.append(fnr)\n",
    "\n",
    "# Set figure size and width of bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define bar width\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set up x values for the bars\n",
    "x = np.arange(len(model_names))\n",
    "\n",
    "# Plot FNR bars\n",
    "plt.bar(x - bar_width/2, fnr_scores, width=bar_width, color='lightsteelblue', label='False Negative Rate')\n",
    "\n",
    "# Plot FPR bars\n",
    "plt.bar(x + bar_width/2, fpr_scores, width=bar_width, color='thistle', label='False Positive Rate')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylabel('Rate', fontsize=14)\n",
    "plt.title('False Negative Rate and False Positive Rate of Each Model \\nWhen At Least One Other Organ Injured', fontsize=14)\n",
    "plt.ylim(0,1)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(x, model_names, rotation=45, fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels to each bar\n",
    "for i, score in enumerate(fnr_scores):\n",
    "    plt.text(i - bar_width/2, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=14)\n",
    "\n",
    "for i, score in enumerate(fpr_scores):\n",
    "    plt.text(i + bar_width/2, score + 0.01, f'{score:.2f}', ha='center', va='bottom', fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4cc02-43e3-4d79-a22e-16a376136f07",
   "metadata": {},
   "source": [
    "### No other injury:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6de93-d120-47f1-b1ef-191cfd530341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataframes for injury_situation = 0 or 2 (other organs healthy)\n",
    "injury_situation_values = [0, 2]\n",
    "\n",
    "organs_healthy_label_dfs = {}\n",
    "organs_healthy_prob_dfs = {}\n",
    "organs_healthy_pred_dfs = {}\n",
    "\n",
    "for model_name, label_df in interpretability_label_dfs.items():\n",
    "    organs_healthy_label_dfs[model_name] = label_df[label_df['injury_situation'].isin(injury_situation_values)]\n",
    "\n",
    "for model_name, prob_df in interpretability_prob_dfs.items():\n",
    "    organs_healthy_prob_dfs[model_name] = prob_df[prob_df['injury_situation'].isin(injury_situation_values)]\n",
    "\n",
    "for model_name, pred_df in interpretability_pred_dfs.items():\n",
    "    organs_healthy_pred_dfs[model_name] = pred_df[pred_df['injury_situation'].isin(injury_situation_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b1bb4-3481-4430-ba02-88066cf78610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "for i, (model_name, label_df) in enumerate(organs_healthy_label_dfs.items()):\n",
    "    total_samples = len(label_df)\n",
    "    \n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    prob_df = organs_healthy_prob_dfs[model_name]\n",
    "    pred_df = organs_healthy_pred_dfs[model_name]\n",
    "\n",
    "    # Calculate AUC score\n",
    "    fprs, tprs, _ = roc_curve(label_df['Labels'], prob_df['Probabilities'])\n",
    "    auc_score = auc(fprs, tprs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.plot(fprs, tprs, color=colors[i], lw=2, label='{} (AUC = {:.2f})'.format(model_name, auc_score))\n",
    "\n",
    "# Add labels and titles for ROC curve\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affda70b-76d7-40a6-82e0-b935a04c1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUPRC Curve\n",
    "for i, (model_name, label_df) in enumerate(organs_healthy_label_dfs.items()):\n",
    "    total_samples = len(label_df)\n",
    "    \n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    prob_df = organs_healthy_prob_dfs[model_name]\n",
    "    pred_df = organs_healthy_pred_dfs[model_name]\n",
    "\n",
    "    # Calculate Precision and Recall\n",
    "    precision, recall, _ = precision_recall_curve(label_df['Labels'], prob_df['Probabilities'])\n",
    "    auprc_score = average_precision_score(label_df['Labels'], prob_df['Probabilities'])\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    plt.step(recall, precision, color=colors[i], where='post', label='{} (AUPRC = {:.2f})'.format(model_name, auprc_score))\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.1, color=colors[i])\n",
    "\n",
    "# Add labels and titles for Precision-Recall curve\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557109ac-bfec-43c1-9c96-84450d5a9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score\n",
    "f1_scores = []\n",
    "model_names = []\n",
    "\n",
    "# Iterate over model names and label dataframes\n",
    "for model_name, label_df in organs_healthy_label_dfs.items():\n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    pred_df = organs_healthy_pred_dfs[model_name]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(label_df['Labels'], pred_df['Predictions'], average='binary')\n",
    "    \n",
    "    # Append F1 score and model name to lists\n",
    "    f1_scores.append(f1)\n",
    "    model_names.append(model_name)\n",
    "\n",
    "# Plot bar plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "bars = plt.bar(model_names, f1_scores, color=colors)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score of Each Model')\n",
    "plt.ylim(0, 0.5)\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01, f'{score:.2f}', ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e0a99f-72bf-47a7-af0b-3a59faf28d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPR\n",
    "fpr_scores = []\n",
    "fnr_scores = []\n",
    "\n",
    "\n",
    "# Calculate FPR for each model\n",
    "for model_name, label_df in organs_healthy_label_dfs.items():\n",
    "    # Retrieve probability and prediction dataframes based on model name\n",
    "    pred_df = organs_healthy_pred_dfs[model_name]\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(label_df['Labels'], pred_df['Predictions']).ravel()\n",
    "    \n",
    "    # Calculate FPR\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "    \n",
    "    # Append FPR to the list\n",
    "    fpr_scores.append(fpr)\n",
    "    fnr_scores.append(fnr)\n",
    "\n",
    "# Set figure size and width of bars\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Define bar width\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set up x values for the bars\n",
    "x = np.arange(len(model_names))\n",
    "\n",
    "# Plot FNR bars\n",
    "plt.bar(x - bar_width/2, fnr_scores, width=bar_width, color='lightsteelblue', label='False Negative Rate')\n",
    "\n",
    "# Plot FPR bars\n",
    "plt.bar(x + bar_width/2, fpr_scores, width=bar_width, color='thistle', label='False Positive Rate')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('False Negative Rate and False Positive Rate of Each Model')\n",
    "plt.ylim(0,0.55)\n",
    "plt.xticks(x, model_names)\n",
    "plt.legend()\n",
    "\n",
    "# Add value labels to each bar\n",
    "for i, score in enumerate(fnr_scores):\n",
    "    plt.text(i - bar_width/2, score + 0.01, f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "for i, score in enumerate(fpr_scores):\n",
    "    plt.text(i + bar_width/2, score + 0.01, f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ef830-5a1c-4a0f-b526-1d4404e77811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50279b-9afe-4235-9daf-03a96a3c368f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
